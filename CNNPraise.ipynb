{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1r5B9FDTV_UQ76k1gxjjfgEM9wBe6B4Y8",
      "authorship_tag": "ABX9TyNnKj7xrvPIna/OQR11yNm4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anu90dl/NeuralNetwork/blob/main/CNNPraise.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "K8szSnYohhvv"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.autograd import Variable"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#download MNIST dataset into pytorch\n",
        "mean_gray = 0.1307\n",
        "stddev_gray = 0.3081\n",
        "\n",
        "transform_f = transforms.Compose([transforms.ToTensor(),\n",
        "                          transforms.Normalize((mean_gray,),(stddev_gray,))])\n",
        "\n",
        "train_data = datasets.MNIST(\"/content/drive/MyDrive/Mnist/\",\n",
        "                            train = True,\n",
        "                            download = True,\n",
        "                            transform = transform_f)\n",
        "\n",
        "test_data = datasets.MNIST(\"/content/drive/MyDrive/Mnist/\",\n",
        "                            train = False,\n",
        "                            download = True,\n",
        "                            transform = transform_f)"
      ],
      "metadata": {
        "id": "KLkyS89tipR6"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Train loader and test loader\n",
        "batch_size = 100\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset = train_data,\n",
        "                                           batch_size = batch_size,\n",
        "                                           shuffle = True)\n",
        "\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset = test_data,\n",
        "                                           batch_size = batch_size,\n",
        "                                           shuffle = False)"
      ],
      "metadata": {
        "id": "QnTWhUBK0yRb"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create the NN\n",
        "\n",
        "class NN(nn.Module):\n",
        "\n",
        "  #init all the batch\n",
        "  def __init__(self):\n",
        "      super(NN,self).__init__()\n",
        "      #padding needed = (kernelsize - 1)/2 , (3-1)/2\n",
        "      self.conv1 = nn.Conv2d(in_channels = 1,out_channels = 8, kernel_size = 3, stride = 1, padding = 1)\n",
        "      self.batchnorm1 = nn.BatchNorm2d(8)\n",
        "      self.relu = nn.ReLU()\n",
        "      self.maxpool1 = nn.MaxPool2d(kernel_size = 2)\n",
        "      # after maxpool the size will be inputsize/2 (28/2) = 14\n",
        "      self.conv2 = nn.Conv2d(in_channels=8,out_channels = 32, kernel_size = 5, stride =1, padding = 2)\n",
        "      self.batchnorm2 = nn.BatchNorm2d(32)\n",
        "      # after maxpool output will be inputsize/2 (14/2)= 7\n",
        "      # the layer will be 7*7*32 = 1568\n",
        "      self.fc1 = nn.Linear(1568,600)\n",
        "      self.droput = nn.Dropout(p=0.5)\n",
        "      self.fc2 = nn.Linear(600,10)\n",
        "\n",
        "  def forward(self,x):\n",
        "      out = self.conv1(x)\n",
        "      out = self.batchnorm1(out)\n",
        "      out = self.relu(out)\n",
        "      out = self.maxpool1(out)\n",
        "      out = self.conv2(out)\n",
        "      out = self.batchnorm2(out)\n",
        "      out = self.relu(out)\n",
        "      out = self.maxpool1(out)\n",
        "      out = out.view(-1,1568)\n",
        "      out = self.fc1(out)\n",
        "      out = self.relu(out)\n",
        "      out = self.droput(out)\n",
        "      out = self.fc2(out)\n",
        "\n",
        "      return out\n"
      ],
      "metadata": {
        "id": "25SnpwHF1ozx"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create the loss and optimizer function\n",
        "model = NN()\n",
        "CUDA = torch.cuda.is_available()\n",
        "\n",
        "if CUDA:\n",
        "  model = model.cuda()\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(),lr = 0.1)"
      ],
      "metadata": {
        "id": "d4q1S9qK7uif"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training iterations\n",
        "train_loss = []\n",
        "test_loss = []\n",
        "train_correct = []\n",
        "test_correct = []\n",
        "epochs = 10\n",
        "\n",
        "for epoc in range(epochs):\n",
        "  iterations = 0\n",
        "  iter_loss = 0.0\n",
        "  correct = 0\n",
        "  test_iterations = 0\n",
        "  test_iter_loss = 0\n",
        "  t_correct = 0\n",
        "\n",
        "\n",
        "  for i,(inputs,labels) in enumerate(train_loader):\n",
        "    if CUDA:\n",
        "      inputs = inputs.cuda()\n",
        "      labels = labels.cuda()\n",
        "      # run the input through the model, batch training (batch size 100)\n",
        "    outputs = model(inputs)\n",
        "    # take the predictions\n",
        "    _, predicted = torch.max(outputs,1)\n",
        "    #check the accuracy\n",
        "    correct += (predicted == labels).sum().item()\n",
        "\n",
        "    # calculate the loss\n",
        "    loss = loss_fn(outputs,labels)\n",
        "    #this iteration loss\n",
        "    iter_loss += loss.item()\n",
        "    # clear the gradient\n",
        "    optimizer.zero_grad()\n",
        "    #backpropagate\n",
        "    loss.backward()\n",
        "    #calculate the new weights\n",
        "    optimizer.step()\n",
        "\n",
        "    #count iterations\n",
        "    iterations += 1\n",
        "\n",
        "  train_loss.append(iter_loss/iterations)\n",
        "  train_correct.append((correct/len(train_loader)))\n",
        "\n",
        "# put model into evaluation mode\n",
        "  model.eval()\n",
        "\n",
        "# check the testing loss\n",
        "  for i,(test_inputs,test_labels) in enumerate(test_loader):\n",
        "    if CUDA:\n",
        "      test_inputs = test_inputs.cuda()\n",
        "      test_labels = test_labels.cuda()\n",
        "\n",
        "    test_outputs = model(test_inputs)\n",
        "    _, test_predictions = torch.max(test_outputs,1)\n",
        "    t_correct += (test_predictions == test_labels).sum().item()\n",
        "    test_iter_loss += loss_fn(test_outputs,test_labels)\n",
        "\n",
        "    test_iterations += 1\n",
        "\n",
        "  test_loss.append(test_iter_loss/test_iterations)\n",
        "  test_correct.append((t_correct/ len(test_loader)))\n",
        "\n",
        "\n",
        "  print(\"Epochs - {}/{} , train loss-{:.3f}, train accuracy-{:.3f},test loss-{:.3f},test accuracy-{:.3f}\".format(epoc +1 ,epochs,train_loss[-1],train_correct[-1],test_loss[-1],test_correct[-1]))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bGMtodpQ9qT8",
        "outputId": "7ababf22-7daa-430c-b2bd-69bc5383d5e3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs - 1/10 , train loss-0.199, train accuracy-93.675,test loss-0.050,test accuracy-98.380\n",
            "Epochs - 2/10 , train loss-0.167, train accuracy-95.587,test loss-0.051,test accuracy-98.210\n",
            "Epochs - 3/10 , train loss-0.040, train accuracy-98.753,test loss-0.032,test accuracy-98.860\n",
            "Epochs - 4/10 , train loss-0.027, train accuracy-99.158,test loss-0.037,test accuracy-98.790\n",
            "Epochs - 5/10 , train loss-0.019, train accuracy-99.388,test loss-0.037,test accuracy-98.910\n",
            "Epochs - 6/10 , train loss-0.014, train accuracy-99.558,test loss-0.033,test accuracy-98.990\n",
            "Epochs - 7/10 , train loss-0.011, train accuracy-99.650,test loss-0.032,test accuracy-98.960\n",
            "Epochs - 8/10 , train loss-0.008, train accuracy-99.725,test loss-0.036,test accuracy-98.940\n",
            "Epochs - 9/10 , train loss-0.006, train accuracy-99.820,test loss-0.031,test accuracy-99.080\n",
            "Epochs - 10/10 , train loss-0.004, train accuracy-99.898,test loss-0.030,test accuracy-99.260\n"
          ]
        }
      ]
    }
  ]
}